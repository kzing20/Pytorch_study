{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPs3BSjOvpPZEZogE3fq7+j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 과적합\n","\n","과적합은 학습 데이터에 치중하여 모델이 학습하는 현상으로 새로운 데이터에 대해서 대응을 못하는 문제다. 따라서 딥러닝에서 가장 쉽게 접할 수 있는 유형이지만 개선하기 힘든 문제다."],"metadata":{"id":"PL5XyUl1HmBV"}},{"cell_type":"markdown","source":["### 1. Dropout & Batch Normalization"],"metadata":{"id":"WhGFmC0oVd6g"}},{"cell_type":"code","source":["class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN,self).__init__()\n","\n","    self.feature_extraction = nn.Sequential(nn.Conv2d(3,6,5),\n","                                            nn.BatchNorm2d(6),\n","                                            nn.ReLU(),\n","                                            nn.MaxPool2d(2,2),\n","                                            nn.Conv2d(6,16,5),\n","                                            nn.BatchNorm2d(16),\n","                                            nn.ReLU(),\n","                                            nn.MaxPool2d(2,2)\n","                                            )\n","    self.classifier = nn.Sequential(nn.Linear(512,120),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(0.5), #비활성화 시킬 노드의 비율\n","                                    nn.Linear(120,64),\n","                                    nn.ReLU(),\n","                                    nn.Linear(64,10)\n","\n","    )\n","  def forward(self,x):\n","    x = self.feature_extraction(x)\n","    x = x.view(-1,512)\n","    x = self.classifier(x)\n","\n","    return x\n","\n","net = CNN().to(device) #모델 선언"],"metadata":{"id":"wOC7liv3VkHE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. L2 Regularization"],"metadata":{"id":"fimbQd5FX18v"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay =1e-2)"],"metadata":{"id":"RCqry0LYX4BJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Data Augmentation"],"metadata":{"id":"sUh6GAQrZO3B"}},{"cell_type":"code","source":["import torchvision.transforms as tr\n","import PIL\n","\n","transf = tr.Compose([\n","    tr.ToPILImage(), tr.RandomCrop(60), tr.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.2,hue=0.2),\n","    tr.RandomHorizontalFlip(),\n","    tr.RandomRotation(10),\n","    tr.ToTensor()\n","])"],"metadata":{"id":"mQaugL1JZSK9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Label Smoothing"],"metadata":{"id":"jGtUIZjGHsG6"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"oSQKnVO3HjMD","executionInfo":{"status":"ok","timestamp":1705826089695,"user_tz":-540,"elapsed":4141,"user":{"displayName":"김징징","userId":"01805882568357009174"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class LabelSmoothingLoss(nn.Module):\n","  def __init__(self, classes, smoothing=0.0, dim=-1):\n","    super(LabelSmoothingLoss, self).__init__()\n","    self.confidence = 1.0 - smoothing\n","    self.smoothing = smoothing\n","    self.cls = classes\n","    self.dim = dim\n","\n","  def forward(self, pred, target):\n","    pred = pred.log_softmax(dim=self.dim) #Cross Entropy 부분의 log softmax 미리 계산하기\n","    with torch.no_grad():\n","      true_dist = torch.zeros_like(pred) #예측값과 동일한 크기의 영텐서 만들기\n","      true_dist.fill_(self.smoothing/(self.cls-1)) #alpha/(K-1)을 만들어 줌(alpha/K로 할 수도 있음)\n","      true_dist.scatter_(1,target.data.unsqueeze(1),self.confidence) #(1-alpha)y + alpha/(K-1)\n","    return torch.mean(torch.sum(-true_dist*pred,dim=self.dim)) #Cross Entropy Loss 계산"]},{"cell_type":"code","source":["ls = LabelSmoothingLoss(10, smoothing=0.2)"],"metadata":{"id":"vEJ3WFCkKDx3","executionInfo":{"status":"ok","timestamp":1705826091940,"user_tz":-540,"elapsed":6,"user":{"displayName":"김징징","userId":"01805882568357009174"}}},"execution_count":2,"outputs":[]}]}